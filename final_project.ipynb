{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><b>MULTIMODAL BIOMETRICS USING GAIT AND EAR FEATURES</b></center>\n",
    "## <center><u>Done by:</u></center>\n",
    "- ### K SAI DINESH (CS20B1122)\n",
    "- ### LOKESH REDDY (CS20B1128)\n",
    "- ### RUDHRA (CS20B1104)\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "from scipy.spatial.distance import hamming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gait Classification using SIFT Detector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through each of the test subjects and compare them against the database images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 45.45454545454545%\n"
     ]
    }
   ],
   "source": [
    "gait_distances = []\n",
    "TEST_IMG_DIR = \"Test Images/\"\n",
    "test_imgs = os.listdir(TEST_IMG_DIR)\n",
    "count_i = 1\n",
    "correct_count = 0\n",
    "\n",
    "for t_img in test_imgs:\n",
    "\n",
    "    test_img = cv2.imread(TEST_IMG_DIR + str(t_img))\n",
    "    \n",
    "    DATA_IMG_DIR = \"Database Images/\"\n",
    "    db_imgs = os.listdir(DATA_IMG_DIR)\n",
    "    min_dist = sys.maxsize\n",
    "    count_j = 1\n",
    "    label = 0\n",
    "    \n",
    "    for db_img in db_imgs:\n",
    "\n",
    "        f_name = DATA_IMG_DIR + db_img\n",
    "        act_img = cv2.imread(f_name)\n",
    "        \n",
    "        gray1 = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "        gray2 = cv2.cvtColor(act_img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        \n",
    "        # Detect blob points using SIFT detector.\n",
    "        kp1, des1 = sift.detectAndCompute(gray1, None)\n",
    "        kp2, des2 = sift.detectAndCompute(gray2, None)\n",
    "        \n",
    "        # Find matches.\n",
    "        bf = cv2.BFMatcher()\n",
    "        matches = bf.match(des1, des2)\n",
    "        \n",
    "        # Calculate distance.\n",
    "        distance = sum([match.distance for match in matches]) / len(matches)\n",
    "        \n",
    "        # Updation of distance.\n",
    "        if(min_dist > distance):\n",
    "            min_dist = distance\n",
    "            label = count_j\n",
    "        \n",
    "        count_j += 1\n",
    "    \n",
    "        gait_distances.append(distance)\n",
    "\n",
    "    # Matched.\n",
    "    if(count_i == label):\n",
    "        correct_count += 1\n",
    "        \n",
    "    count_i += 1\n",
    "\n",
    "print(f\"Accuracy : {(correct_count/(len(os.listdir(TEST_IMG_DIR))-1))*100}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gait Classification using Hamming Distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same thing as above but by using Hamming distance as metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 41.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "count_i = 1\n",
    "correct_count = 0\n",
    "TEST_IMG_DIR = \"Test Images/\"\n",
    "\n",
    "for folder in os.listdir(TEST_IMG_DIR):\n",
    "\n",
    "    test_img = cv2.imread(\"Test Images/\"+str(folder))\n",
    "    DATA_IMG_DIR = \"Database Images/\"\n",
    "    db_imgs = os.listdir(DATA_IMG_DIR)\n",
    "    min_dist = sys.maxsize\n",
    "    count_j = 1\n",
    "    label = 0\n",
    "\n",
    "    for db_img in db_imgs:\n",
    "        f_name = DATA_IMG_DIR + db_img\n",
    "        act_img = cv2.imread(f_name)\n",
    "        \n",
    "        # Convert to gray scale.\n",
    "        gray1 = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "        gray2 = cv2.cvtColor(act_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        gray1 = cv2.resize(gray1, (200, 200))\n",
    "        gray2 = cv2.resize(gray2, (200, 200))\n",
    "\n",
    "        \n",
    "        # Flatten them.\n",
    "        gray1 = gray1.flatten()\n",
    "        gray2 = gray2.flatten()\n",
    "\n",
    "        gray1 = list(gray1)\n",
    "        gray2 = list(gray2)\n",
    "\n",
    "        # Calculate Hamming Distance.\n",
    "        distance = hamming(gray1, gray2)\n",
    "        if(min_dist > distance):\n",
    "            min_dist = distance\n",
    "            label = count_j\n",
    "        \n",
    "        count_j += 1\n",
    "    \n",
    "    # Matched.\n",
    "    if(count_i == label):\n",
    "        correct_count += 1\n",
    "        \n",
    "    count_i += 1\n",
    "\n",
    "print(f\"Accuracy : {(correct_count/len(os.listdir(TEST_IMG_DIR)))*100}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Ear distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ear contour detection\n",
    "def get_ear_contour(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    _, thresh = cv2.threshold(blur, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    ear_contour = max(contours, key=cv2.contourArea)\n",
    "    return ear_contour\n",
    "\n",
    "# Step 2: Binarization\n",
    "def binarize_ear(img, ear_contour):\n",
    "    mask = np.zeros(img.shape[:2], np.uint8)\n",
    "    cv2.drawContours(mask, [ear_contour], 0, 255, -1)\n",
    "    ear_pixels = cv2.bitwise_and(img, img, mask=mask)\n",
    "    ear_pixels_gray = cv2.cvtColor(ear_pixels, cv2.COLOR_BGR2GRAY)\n",
    "    _, ear_binary = cv2.threshold(ear_pixels_gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    return ear_binary\n",
    "\n",
    "# Step 3: Coordinates normalization\n",
    "def normalize_ear(ear_binary):\n",
    "    rows, cols = ear_binary.shape[:2]\n",
    "    contours, _ = cv2.findContours(ear_binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(contours) == 2:\n",
    "        ear_contour = contours[1]\n",
    "    else:\n",
    "        ear_contour = max(contours, key=cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(ear_contour)\n",
    "    norm_ear = ear_binary[y:y + h, x:x + w]\n",
    "    norm_ear = cv2.resize(norm_ear, (256, 256))\n",
    "    return norm_ear\n",
    "\n",
    "# Step 4: Feature extraction (2 steps)\n",
    "def extract_features(norm_ear):\n",
    "    # Step 4.1: Get horizontal and vertical projections\n",
    "    vert_proj = np.sum(norm_ear, axis=0)\n",
    "    horiz_proj = np.sum(norm_ear, axis=1)\n",
    "\n",
    "    # Step 4.2: Extract features from projections\n",
    "    features = []\n",
    "    for i in range(5):\n",
    "        x1 = int(i * 20)\n",
    "        x2 = int((i + 1) * 20)\n",
    "        vert_segment = vert_proj[x1:x2]\n",
    "        horiz_segment = horiz_proj[x1:x2]\n",
    "        vert_mean = np.mean(vert_segment)\n",
    "        horiz_mean = np.mean(horiz_segment)\n",
    "        features.append(vert_mean)\n",
    "        features.append(horiz_mean)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Algorithm(IMG_PATH):\n",
    "\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_profileface.xml')\n",
    "    img = cv2.imread(IMG_PATH)\n",
    "    img = cv2.flip(img,1)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the grayscale image\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        img = cv2.flip(img,1)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces in the grayscale image\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "\n",
    "    # Loop over each detected face and extract the ear features\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face region\n",
    "        face_roi = img[y:y+h, x:x+w+50]\n",
    "        # ear_contour = get_ear_contour(face_roi)\n",
    "        # ear_binary = binarize_ear(face_roi, ear_contour)\n",
    "        # norm_ear = normalize_ear(ear_binary)\n",
    "        # ear_features = extract_features(norm_ear)\n",
    "        # return (ear_features)\n",
    "\n",
    "        # Convert the face region to grayscale\n",
    "        gray_face_roi = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Perform edge detection on the grayscale face region\n",
    "        edges = cv2.Canny(gray_face_roi, 50, 200)\n",
    "        \n",
    "        # Apply thresholding to the edge image to extract the ear\n",
    "        _, thresh = cv2.threshold(edges, 50, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # Perform dilation on the thresholded image to fill in gaps\n",
    "        kernel = np.ones((5,5),np.uint8)\n",
    "        dilated = cv2.dilate(thresh, kernel, iterations = 1)\n",
    "        \n",
    "        # Find contours in the dilated image\n",
    "        contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Extract the largest contour as the ear region\n",
    "        if len(contours) > 0:\n",
    "            ear_contour = max(contours, key=cv2.contourArea)\n",
    "            (ex, ey, ew, eh) = cv2.boundingRect(ear_contour)\n",
    "            ear_img = face_roi[ey:ey+eh, ex:ex+ew]\n",
    "            \n",
    "            ear_features = extract_features(ear_img)\n",
    "            return(ear_features)\n",
    "        else:\n",
    "            return (-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Ear features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\hp\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "folder_names = os.listdir(\"ear_photos/\")\n",
    "db_ear_features = {}\n",
    "count = 1\n",
    "for folder in folder_names:\n",
    "    IMG_FILE_PATH = \"ear_photos/\"+str(folder)\n",
    "    db_ear_features[count] = Algorithm(IMG_FILE_PATH)\n",
    "    count += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_ear_subjects = len(os.listdir(\"ear_photos/\"))\n",
    "total_ear_subjects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill te NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,total_ear_subjects+1):\n",
    "    db_ear_features[i] = np.array(db_ear_features[i])\n",
    "    db_ear_features[i][np.isnan(db_ear_features[i])] = np.nanmean(db_ear_features[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the ear extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder_names = os.listdir(\"ear_test_photos/\")\n",
    "test_ear_features = {}\n",
    "count = 1\n",
    "for folder in test_folder_names:\n",
    "    IMG_FILE_PATH = \"ear_test_photos/\"+str(folder)\n",
    "    test_ear_features[count] = Algorithm(IMG_FILE_PATH)\n",
    "    count += 1    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For test ear images to the same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,total_ear_subjects+1):\n",
    "    test_ear_features[i] = np.array(test_ear_features[i])\n",
    "    test_ear_features[i][np.isnan(test_ear_features[i])] = np.nanmean(test_ear_features[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Ear distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 41.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "ear_distances = []\n",
    "correct_count = 0\n",
    "\n",
    "for i in range(1,total_ear_subjects+1):\n",
    "\n",
    "    min_dist = sys.maxsize\n",
    "    label = 0\n",
    "\n",
    "    for j in range(1,13):\n",
    "        curr_dist = abs(np.linalg.norm(test_ear_features[i]-db_ear_features[j]))\n",
    "        if(curr_dist < min_dist):\n",
    "            min_dist = curr_dist\n",
    "            label = j\n",
    "        ear_distances.append(curr_dist)\n",
    "    \n",
    "    if(i == label):\n",
    "        correct_count += 1\n",
    "\n",
    "print(f\"Accuracy : {(correct_count/len(test_ear_features))*100}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining both Gait and Ear pateerns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "gait_distances = gait_distances/np.linalg.norm(gait_distances)\n",
    "ear_distances = ear_distances/np.linalg.norm(ear_distances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max and Min ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.18320709849192374\n",
      "0.06084505070314935 0.099773616503489\n"
     ]
    }
   ],
   "source": [
    "print(min(ear_distances),max(ear_distances))\n",
    "print(min(gait_distances),max(gait_distances))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine ear and gait distances using the weighted sum by using Alpha and Beta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Accuracy : 58.33%\n"
     ]
    }
   ],
   "source": [
    "total_subjects = int(np.sqrt(len(gait_distances)))\n",
    "alpha = 0\n",
    "step = 0.01\n",
    "correct_count = 0\n",
    "highest_acc = 0\n",
    "\n",
    "while alpha <= 1:\n",
    "    beta = 0\n",
    "    while beta <= 1:\n",
    "        correct_count = 0\n",
    "        for i in range(total_subjects):\n",
    "\n",
    "            start_ind = i*total_subjects\n",
    "            label = 0\n",
    "            min_dist = sys.maxsize\n",
    "            \n",
    "            for j in range(start_ind, start_ind + total_subjects):\n",
    "                dist = (alpha * gait_distances[j]) + (beta * ear_distances[j])\n",
    "                if(min_dist > dist):\n",
    "                    min_dist = dist\n",
    "                    label = j - start_ind\n",
    "            \n",
    "            if(i == label):\n",
    "                correct_count += 1\n",
    "        \n",
    "        highest_acc = max(highest_acc,correct_count/total_subjects)\n",
    "        beta += step\n",
    "    \n",
    "    alpha += step\n",
    "\n",
    "print(f\"Highest Accuracy : {round(highest_acc*100,2)}%\")\n",
    "# print(f\"Accuracy : {round((correct_count/total_subjects)*100,2)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>---End of Notebook---</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
